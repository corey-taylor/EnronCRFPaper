/* Copyright (C) 2003 University of Pennsylvania.
   This file is part of "MALLET" (MAchine Learning for LanguagE Toolkit).
http://www.cs.umass.edu/~mccallum/mallet
This software is provided under the terms of the Common Public License,
version 1.0, as published by http://www.opensource.org.  For further
information, see the file `LICENSE' included with this distribution. */

package cc.mallet.fst;

import info.monitorenter.gui.chart.Chart2D;
import info.monitorenter.gui.chart.ITrace2D;
import info.monitorenter.gui.chart.traces.Trace2DSimple;

import java.awt.Color;
import java.awt.Frame;
import java.awt.event.WindowAdapter;
import java.awt.event.WindowEvent;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.InputStreamReader;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.Reader;
import java.util.ArrayList;
import java.util.Formatter;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Locale;
import java.util.Map;
import java.util.Random;
import java.util.TreeSet;
import java.util.logging.Logger;
import java.util.regex.Pattern;

import javax.swing.JFrame;
import javax.swing.JOptionPane;

import cc.mallet.topics.ParallelTopicModel;
import cc.mallet.types.Alphabet;
import cc.mallet.types.AugmentableFeatureVector;
import cc.mallet.types.FeatureSequence;
import cc.mallet.types.FeatureVector;
import cc.mallet.types.FeatureVectorSequence;
import cc.mallet.types.IDSorter;
import cc.mallet.types.Instance;
import cc.mallet.types.InstanceList;
import cc.mallet.types.LabelAlphabet;
import cc.mallet.types.LabelSequence;
import cc.mallet.types.Sequence;
import cc.mallet.pipe.CharSequence2TokenSequence;
import cc.mallet.pipe.CharSequenceLowercase;
import cc.mallet.pipe.Pipe;
import cc.mallet.pipe.SerialPipes;
import cc.mallet.pipe.TokenSequence2FeatureSequence;
import cc.mallet.pipe.TokenSequenceRemoveStopwords;
import cc.mallet.pipe.iterator.CsvIterator;
import cc.mallet.pipe.iterator.LineGroupIterator;
import cc.mallet.util.CommandOption;
import cc.mallet.util.MalletLogger;

/**
 * This class's main method trains, tests, or runs a generic CRF-based
 * sequence tagger.
 * <p>
 * Training and test files consist of blocks of lines, one block for each instance, 
 * separated by blank lines. Each block of lines should have the first form 
 * specified for the input of {@link SimpleTaggerSentence2FeatureVectorSequence}. 
 * A variety of command line options control the operation of the main program, as
 * described in the comments for {@link #main main}.
 *
 * @author Fernando Pereira <a href="mailto:pereira@cis.upenn.edu">pereira@cis.upenn.edu</a>
 * @version 1.0
 */
public class test
{

	//************* COREY'S CODE STARTS HERE
	
	InstanceList instances;
	ParallelTopicModel model;
	static String resultText;
	
	// Containers for words extracted from topic model with rank from model, all topics + topic prob and 
	//	highest probability topic + words associated with it
	
	static Map<Integer, String> topicwords = new HashMap<Integer, String>();
	
	//************* COREY'S CODE ENDS HERE AND RESTARTS AT MAIN
	
  private static Logger logger =
    MalletLogger.getLogger(SimpleTagger.class.getName());

  /**
   * No <code>SimpleTagger</code> objects allowed.
   */
  private test()
  {
  }

  /**
   * Converts an external encoding of a sequence of elements with binary
   * features to a {@link FeatureVectorSequence}.  If target processing
   * is on (training or labeled test data), it extracts element labels
   * from the external encoding to create a target {@link LabelSequence}.
   * Two external encodings are supported:
   * <ol>
   *  <li> A {@link String} containing lines of whitespace-separated tokens.</li>
   *  <li> a {@link String}<code>[][]</code>.</li>
   * </ol>
   *
   * Both represent rows of tokens. When target processing is on, the last token
   * in each row is the label of the sequence element represented by
   * this row. All other tokens in the row, or all tokens in the row if
   * not target processing, are the names of features that are on for
   * the sequence element described by the row.
   *           
   */
  public static class SimpleTaggerSentence2FeatureVectorSequence extends Pipe
  {
    // gdruck
    // Previously, there was no serialVersionUID.  This is ID that would 
    // have been automatically generated by the compiler.  Therefore,
    // other changes should not break serialization.  
    private static final long serialVersionUID = -2059308802200728625L;

    /**
     * Creates a new
     * <code>SimpleTaggerSentence2FeatureVectorSequence</code> instance.
     */
    public SimpleTaggerSentence2FeatureVectorSequence ()
    {
      super (new Alphabet(), new LabelAlphabet());
    }

    /**
     * Parses a string representing a sequence of rows of tokens into an
     * array of arrays of tokens.
     *
     * @param sentence a <code>String</code>
     * @return the corresponding array of arrays of tokens.
     */
    private String[][] parseSentence(String sentence)
    {
      String[] lines = sentence.split("\n");
      String[][] tokens = new String[lines.length][];
      for (int i = 0; i < lines.length; i++)
        tokens[i] = lines[i].split(" ");
      return tokens;
    }

    public Instance pipe (Instance carrier)
    {
      Object inputData = carrier.getData();
      Alphabet features = getDataAlphabet();
      LabelAlphabet labels;
      LabelSequence target = null;
      String [][] tokens;
      if (inputData instanceof String)
        tokens = parseSentence((String)inputData);
      else if (inputData instanceof String[][])
        tokens = (String[][])inputData;
      else
        throw new IllegalArgumentException("Not a String or String[][]; got "+inputData);
      FeatureVector[] fvs = new FeatureVector[tokens.length];
      if (isTargetProcessing())
      {
        labels = (LabelAlphabet)getTargetAlphabet();
        target = new LabelSequence (labels, tokens.length);
      }
      for (int l = 0; l < tokens.length; l++) {
        int nFeatures;
        if (isTargetProcessing())
        {
          if (tokens[l].length < 1)
            throw new IllegalStateException ("Missing label at line " + l + " instance "+carrier.getName ());
          nFeatures = tokens[l].length - 1;
          target.add(tokens[l][nFeatures]);
        }
        else nFeatures = tokens[l].length;
        ArrayList<Integer> featureIndices = new ArrayList<Integer>();
        for (int f = 0; f < nFeatures; f++) {
        	int featureIndex = features.lookupIndex(tokens[l][f]);
        	// gdruck
        	// If the data alphabet's growth is stopped, featureIndex
        	// will be -1.  Ignore these features.
        	if (featureIndex >= 0) {
        		featureIndices.add(featureIndex);
        	}
        }
        int[] featureIndicesArr = new int[featureIndices.size()];
        for (int index = 0; index < featureIndices.size(); index++) {
        	featureIndicesArr[index] = featureIndices.get(index);
        }
       	fvs[l] = featureInductionOption.value ? new AugmentableFeatureVector(features, featureIndicesArr, null, featureIndicesArr.length) : 
        	new FeatureVector(features, featureIndicesArr);
      }
      carrier.setData(new FeatureVectorSequence(fvs));
      if (isTargetProcessing())
        carrier.setTarget(target);
      else
        carrier.setTarget(new LabelSequence(getTargetAlphabet()));
      return carrier;
    }
  }

  private static final CommandOption.Double gaussianVarianceOption = new CommandOption.Double
    (SimpleTagger.class, "gaussian-variance", "DECIMAL", true, 10.0,
     "The gaussian prior variance used for training.", null);

  private static final CommandOption.Boolean trainOption = new CommandOption.Boolean
    (SimpleTagger.class, "train", "true|false", true, false,
     "Whether to train", null);

  private static final CommandOption.String testOption = new CommandOption.String
    (SimpleTagger.class, "test", "lab or seg=start-1.continue-1,...,start-n.continue-n",
     true, null,
     "Test measuring labeling or segmentation (start-i, continue-i) accuracy", null);

  private static final CommandOption.File modelOption = new CommandOption.File
    (SimpleTagger.class, "model-file", "FILENAME", true, null,
     "The filename for reading (train/run) or saving (train) the model.", null);

  private static final CommandOption.Double trainingFractionOption = new CommandOption.Double
    (SimpleTagger.class, "training-proportion", "DECIMAL", true, 0.5,
     "Fraction of data to use for training in a random split.", null);

  private static final CommandOption.Integer randomSeedOption = new CommandOption.Integer
    (SimpleTagger.class, "random-seed", "INTEGER", true, 0,
     "The random seed for randomly selecting a proportion of the instance list for training", null);

  private static final CommandOption.IntegerArray ordersOption = new CommandOption.IntegerArray
    (SimpleTagger.class, "orders", "COMMA-SEP-DECIMALS", true, new int[]{1},
     "List of label Markov orders (main and backoff) ", null);

  private static final CommandOption.String forbiddenOption = new CommandOption.String(
      SimpleTagger.class, "forbidden", "REGEXP", true,
      "\\s", "label1,label2 transition forbidden if it matches this", null);

  private static final CommandOption.String allowedOption = new CommandOption.String(
      SimpleTagger.class, "allowed", "REGEXP", true,
      ".*", "label1,label2 transition allowed only if it matches this", null);

  private static final CommandOption.String defaultOption = new CommandOption.String(
      SimpleTagger.class, "default-label", "STRING", true, "O",
      "Label for initial context and uninteresting tokens", null);

  private static final CommandOption.Integer iterationsOption = new CommandOption.Integer(
      SimpleTagger.class, "iterations", "INTEGER", true, 500,
      "Number of training iterations", null);

  private static final CommandOption.Boolean viterbiOutputOption = new CommandOption.Boolean(
      SimpleTagger.class, "viterbi-output", "true|false", true, false,
      "Print Viterbi periodically during training", null);

  private static final CommandOption.Boolean connectedOption = new CommandOption.Boolean(
      SimpleTagger.class, "fully-connected", "true|false", true, true,
      "Include all allowed transitions, even those not in training data", null);
  
  private static final CommandOption.String weightsOption = new CommandOption.String(
      SimpleTagger.class, "weights", "sparse|some-dense|dense", true, "some-dense",
      "Use sparse, some dense (using a heuristic), or dense features on transitions.", null);

  private static final CommandOption.Boolean continueTrainingOption = new CommandOption.Boolean(
      SimpleTagger.class, "continue-training", "true|false", false, false,
      "Continue training from model specified by --model-file", null);

  private static final CommandOption.Integer nBestOption = new CommandOption.Integer(
      SimpleTagger.class, "n-best", "INTEGER", true, 1,
      "How many answers to output", null);

  private static final CommandOption.Integer cacheSizeOption = new CommandOption.Integer(
      SimpleTagger.class, "cache-size", "INTEGER", true, 100000,
      "How much state information to memoize in n-best decoding", null);

  private static final CommandOption.Boolean includeInputOption = new CommandOption.Boolean(
          SimpleTagger.class, "include-input", "true|false", true, false,
     "Whether to include the input features when printing decoding output", null);

  private static final CommandOption.Boolean featureInductionOption = new CommandOption.Boolean(
          SimpleTagger.class, "feature-induction", "true|false", true, false,
     "Whether to perform feature induction during training", null);
  
  private static final CommandOption.Integer numThreads = new CommandOption.Integer(
      SimpleTagger.class, "threads", "INTEGER", true, 1,
      "Number of threads to use for CRF training.", null);

  private static final CommandOption.List commandOptions =
    new CommandOption.List (
        "Training, testing and running a generic tagger.",
        new CommandOption[] {
          gaussianVarianceOption,
          trainOption,
          iterationsOption,
          testOption,
          trainingFractionOption,
          modelOption,
          randomSeedOption,
          ordersOption,
          forbiddenOption,
          allowedOption,
          defaultOption,
          viterbiOutputOption,
          connectedOption,
          weightsOption,
          continueTrainingOption,
          nBestOption,
          cacheSizeOption,
          includeInputOption,
          featureInductionOption,
          numThreads
        });

  /**
   * Create and train a CRF model from the given training data,
   * optionally testing it on the given test data.
   *
   * @param training training data
   * @param testing test data (possibly <code>null</code>)
   * @param eval accuracy evaluator (possibly <code>null</code>)
   * @param orders label Markov orders (main and backoff)
   * @param defaultLabel default label
   * @param forbidden regular expression specifying impossible label
   * transitions <em>current</em><code>,</code><em>next</em>
   * (<code>null</code> indicates no forbidden transitions)
   * @param allowed regular expression specifying allowed label transitions
   * (<code>null</code> indicates everything is allowed that is not forbidden)
   * @param connected whether to include even transitions not
   * occurring in the training data.
   * @param iterations number of training iterations
   * @param var Gaussian prior variance
   * @return the trained model
   */
  public static CRF train(InstanceList training, InstanceList testing,
      TransducerEvaluator eval, int[] orders,
      String defaultLabel,
      String forbidden, String allowed,
      boolean connected, int iterations, double var, CRF crf)
  {
    Pattern forbiddenPat = Pattern.compile(forbidden);
    Pattern allowedPat = Pattern.compile(allowed);
    if (crf == null) {
      crf = new CRF(training.getPipe(), (Pipe)null);
      String startName =
        crf.addOrderNStates(training, orders, null,
            defaultLabel, forbiddenPat, allowedPat,
            connected);
      for (int i = 0; i < crf.numStates(); i++)
        crf.getState(i).setInitialWeight (Transducer.IMPOSSIBLE_WEIGHT);
      crf.getState(startName).setInitialWeight(0.0);
    }
    logger.info("Training on " + training.size() + " instances");
    if (testing != null)
      logger.info("Testing on " + testing.size() + " instances");
    
  	assert(numThreads.value > 0);
    if (numThreads.value > 1) {
      CRFTrainerByThreadedLabelLikelihood crft = new CRFTrainerByThreadedLabelLikelihood (crf,numThreads.value);
      crft.setGaussianPriorVariance(var);
      
      if (weightsOption.value.equals("dense")) {
        crft.setUseSparseWeights(false);
        crft.setUseSomeUnsupportedTrick(false);
      }
      else if (weightsOption.value.equals("some-dense")) {
        crft.setUseSparseWeights(true);
        crft.setUseSomeUnsupportedTrick(true);
      }
      else if (weightsOption.value.equals("sparse")) {
        crft.setUseSparseWeights(true);
        crft.setUseSomeUnsupportedTrick(false);
      }
      else {
        throw new RuntimeException("Unknown weights option: " + weightsOption.value);
      }
      
      if (featureInductionOption.value) {
      	throw new IllegalArgumentException("Multi-threaded feature induction is not yet supported.");
      } else {
      	boolean converged;
      	for (int i = 1; i <= iterations; i++) {
      		converged = crft.train (training, 1);
      		if (i % 1 == 0 && eval != null) // Change the 1 to higher integer to evaluate less often
      			eval.evaluate(crft);
      		if (viterbiOutputOption.value && i % 10 == 0)
      			new ViterbiWriter("", new InstanceList[] {training, testing}, new String[] {"training", "testing"}).evaluate(crft);
      		if (converged)
      			break;
      	}
      }
      crft.shutdown();
    }
    else {
      CRFTrainerByLabelLikelihood crft = new CRFTrainerByLabelLikelihood (crf);
      crft.setGaussianPriorVariance(var);
      
      if (weightsOption.value.equals("dense")) {
        crft.setUseSparseWeights(false);
        crft.setUseSomeUnsupportedTrick(false);
      }
      else if (weightsOption.value.equals("some-dense")) {
        crft.setUseSparseWeights(true);
        crft.setUseSomeUnsupportedTrick(true);
      }
      else if (weightsOption.value.equals("sparse")) {
        crft.setUseSparseWeights(true);
        crft.setUseSomeUnsupportedTrick(false);
      }
      else {
        throw new RuntimeException("Unknown weights option: " + weightsOption.value);
      }
      
      if (featureInductionOption.value) {
      	 crft.trainWithFeatureInduction(training, null, testing, eval, iterations, 10, 20, 500, 0.5, false, null);
      } else {
      	boolean converged;
      	for (int i = 1; i <= iterations; i++) {
      		converged = crft.train (training, 1);
      		if (i % 1 == 0 && eval != null) // Change the 1 to higher integer to evaluate less often
      			eval.evaluate(crft);
      		if (viterbiOutputOption.value && i % 10 == 0)
      			new ViterbiWriter("", new InstanceList[] {training, testing}, new String[] {"training", "testing"}).evaluate(crft);
      		if (converged)
      			break;
      	}
      }
    }
    
    

    return crf;
  }

  /**
   * Test a transducer on the given test data, evaluating accuracy
   * with the given evaluator
   *
   * @param model a <code>Transducer</code>
   * @param eval accuracy evaluator
   * @param testing test data
   */
  public static void test(TransducerTrainer tt, TransducerEvaluator eval,
      InstanceList testing)
  {
    eval.evaluateInstanceList(tt, testing, "Testing");
  }

  /**
   * Apply a transducer to an input sequence to produce the k highest-scoring
   * output sequences.
   *
   * @param model the <code>Transducer</code>
   * @param input the input sequence
   * @param k the number of answers to return
   * @return array of the k highest-scoring output sequences
   */
  public static Sequence[] apply(Transducer model, Sequence input, int k)
  {
    Sequence[] answers;
    if (k == 1) {
      answers = new Sequence[1];
      answers[0] = model.transduce (input);
    }
    else {
      MaxLatticeDefault lattice =
              new MaxLatticeDefault (model, input, null, cacheSizeOption.value());

      answers = lattice.bestOutputSequences(k).toArray(new Sequence[0]);
    }
    return answers;
  }

  /**
   * Command-line wrapper to train, test, or run a generic CRF-based tagger.
   *
   * @param args the command line arguments. Options (shell and Java quoting should be added as needed):
   *<dl>
   *<dt><code>--help</code> <em>boolean</em></dt>
   *<dd>Print this command line option usage information.  Give <code>true</code> for longer documentation. Default is <code>false</code>.</dd>
   *<dt><code>--prefix-code</code> <em>Java-code</em></dt>
   *<dd>Java code you want run before any other interpreted code.  Note that the text is interpreted without modification, so unlike some other Java code options, you need to include any necessary 'new's. Default is null.</dd>
   *<dt><code>--gaussian-variance</code> <em>positive-number</em></dt>
   *<dd>The Gaussian prior variance used for training. Default is 10.0.</dd>
   *<dt><code>--train</code> <em>boolean</em></dt>
   *<dd>Whether to train. Default is <code>false</code>.</dd>
   *<dt><code>--iterations</code> <em>positive-integer</em></dt>
   *<dd>Number of training iterations. Default is 500.</dd>
   *<dt><code>--test</code> <code>lab</code> or <code>seg=</code><em>start-1</em><code>.</code><em>continue-1</em><code>,</code>...<code>,</code><em>start-n</em><code>.</code><em>continue-n</em></dt>
   *<dd>Test measuring labeling or segmentation (<em>start-i</em>, <em>continue-i</em>) accuracy. Default is no testing.</dd>
   *<dt><code>--training-proportion</code> <em>number-between-0-and-1</em></dt>
   *<dd>Fraction of data to use for training in a random split. Default is 0.5.</dd>
   *<dt><code>--model-file</code> <em>filename</em></dt>
   *<dd>The filename for reading (train/run) or saving (train) the model. Default is null.</dd>
   *<dt><code>--random-seed</code> <em>integer</em></dt>
   *<dd>The random seed for randomly selecting a proportion of the instance list for training Default is 0.</dd>
   *<dt><code>--orders</code> <em>comma-separated-integers</em></dt>
   *<dd>List of label Markov orders (main and backoff)  Default is 1.</dd>
   *<dt><code>--forbidden</code> <em>regular-expression</em></dt>
   *<dd>If <em>label-1</em><code>,</code><em>label-2</em> matches the expression, the corresponding transition is forbidden. Default is <code>\\s</code> (nothing forbidden).</dd>
   *<dt><code>--allowed</code> <em>regular-expression</em></dt>
   *<dd>If <em>label-1</em><code>,</code><em>label-2</em> does not match the expression, the corresponding expression is forbidden. Default is <code>.*</code> (everything allowed).</dd>
   *<dt><code>--default-label</code> <em>string</em></dt>
   *<dd>Label for initial context and uninteresting tokens. Default is <code>O</code>.</dd>
   *<dt><code>--viterbi-output</code> <em>boolean</em></dt>
   *<dd>Print Viterbi periodically during training. Default is <code>false</code>.</dd>
   *<dt><code>--fully-connected</code> <em>boolean</em></dt>
   *<dd>Include all allowed transitions, even those not in training data. Default is <code>true</code>.</dd>
   *<dt><code>--weights</code> <em>sparse|some-dense|dense</em></dt>
   *<dd>Create sparse, some dense (using a heuristic), or dense features on transitions. Default is <code>some-dense</code>.</dd>
   *<dt><code>--n-best</code> <em>positive-integer</em></dt>
   *<dd>Number of answers to output when applying model. Default is 1.</dd>
   *<dt><code>--include-input</code> <em>boolean</em></dt>
   *<dd>Whether to include input features when printing decoding output. Default is <code>false</code>.</dd>
   *<dt><code>--threads</code> <em>positive-integer</em></dt>
   *<dd>Number of threads for CRF training. Default is 1.</dd>
   *</dl>
   * Remaining arguments:
   *<ul>
   *<li><em>training-data-file</em> if training </li>
   *<li><em>training-and-test-data-file</em>, if training and testing with random split</li>
   *<li><em>training-data-file</em> <em>test-data-file</em> if training and testing from separate files</li>
   *<li><em>test-data-file</em> if testing</li>
   *<li><em>input-data-file</em> if applying to new data (unlabeled)</li>
   *</ul>
   * @exception Exception if an error occurs
   */
  public static void main (String[] args) throws Exception
  {
//************	COREY'S CODE STARTS AGAIN HERE	************//
	  
	//************	1.	CRF 
	  
	  //	Initialise ArrayLists to hold speech types slacker; aggressive; panic
	  //	For use in building plots later...
	  
	  ArrayList<Double> sla = new ArrayList<Double>();
	  ArrayList<Double> agg = new ArrayList<Double>();
	  ArrayList<Double> pan = new ArrayList<Double>();
	  
	  //	Arratlist containing historical Enron stock data
	  
	  ArrayList<Double> stock = new ArrayList<Double>();
	  
	  stock.add(43.44);
	  stock.add(64.50);
	  stock.add(69.00);
	  stock.add(73.56);
	  stock.add(72.31);
	  stock.add(71.13);
	  stock.add(68.00);
	  stock.add(76.00);
	  stock.add(85.33);
	  stock.add(86.44);
	  stock.add(83.25);
	  stock.add(65.50);
	  stock.add(79.88);
	  stock.add(78.79);
	  stock.add(68.68);
	  stock.add(56.57);
	  stock.add(62.41);
	  stock.add(53.04);
	  stock.add(48.30);
	  stock.add(45.61);
	  stock.add(35.00);
	  stock.add(29.15);
	  stock.add(11.99);
	  stock.add(0.40);
	  
	  String email = new String("");
	  
	  for (int f = 0; f < 45; f++)
      {
	  
		  double Sla=0.0, Agg=0.0, Pan=0.0;
		  
    Reader trainingFile = null, testFile = null;
    InstanceList trainingData = null, testData = null;
    int numEvaluations = 0;
    int iterationsBetweenEvals = 16;
    int restArgs = commandOptions.processOptions(args);
    if (restArgs == args.length)
    {
      commandOptions.printUsage(true);
      throw new IllegalArgumentException("Missing data file(s)");
    }
    if (trainOption.value)
    {
      trainingFile = new FileReader(new File(args[restArgs]));
      if (testOption.value != null && restArgs < args.length - 1)
        testFile = new FileReader(new File(email+f));
    } else 
      testFile = new FileReader(new File(email+f));

    Pipe p = null;
    CRF crf = null;
    TransducerEvaluator eval = null;
    if (continueTrainingOption.value || !trainOption.value) {
      if (modelOption.value == null)
      {
        commandOptions.printUsage(true);
        throw new IllegalArgumentException("Missing model file option");
      }
      ObjectInputStream s =
        new ObjectInputStream(new FileInputStream(modelOption.value));
      crf = (CRF) s.readObject();
      s.close();
      p = crf.getInputPipe();
    }
    else {
      p = new SimpleTaggerSentence2FeatureVectorSequence();
      p.getTargetAlphabet().lookupIndex(defaultOption.value);
    }


    if (trainOption.value)
    {
      p.setTargetProcessing(true);
      trainingData = new InstanceList(p);
      trainingData.addThruPipe(
          new LineGroupIterator(trainingFile,
            Pattern.compile("^\\s*$"), true));
      logger.info
        ("Number of features in training data: "+p.getDataAlphabet().size());
      if (testOption.value != null)
      {
        if (testFile != null)
        {
          testData = new InstanceList(p);
          testData.addThruPipe(
              new LineGroupIterator(testFile,
                Pattern.compile("^\\s*$"), true));
        } else
        {
          Random r = new Random (randomSeedOption.value);
          InstanceList[] trainingLists =
            trainingData.split(
                r, new double[] {trainingFractionOption.value,
                  1-trainingFractionOption.value});
          trainingData = trainingLists[0];
          testData = trainingLists[1];
        }
      }
    } else if (testOption.value != null)
    {
      p.setTargetProcessing(true);
      testData = new InstanceList(p);
      testData.addThruPipe(
          new LineGroupIterator(testFile,
            Pattern.compile("^\\s*$"), true));
    } else
    {
      p.setTargetProcessing(false);
      testData = new InstanceList(p);
      testData.addThruPipe(
          new LineGroupIterator(testFile,
            Pattern.compile("^\\s*$"), true));
    }
    //logger.info ("Number of predicates: "+p.getDataAlphabet().size());
    
    
    if (testOption.value != null)
    {
      if (testOption.value.startsWith("lab"))
        eval = new TokenAccuracyEvaluator(new InstanceList[] {trainingData, testData}, new String[] {"Training", "Testing"});
      else if (testOption.value.startsWith("seg="))
      {
        String[] pairs = testOption.value.substring(4).split(",");
        if (pairs.length < 1)
        {
          commandOptions.printUsage(true);
          throw new IllegalArgumentException(
              "Missing segment start/continue labels: " + testOption.value);
        }
        String startTags[] = new String[pairs.length];
        String continueTags[] = new String[pairs.length];
        for (int i = 0; i < pairs.length; i++)
        {
          String[] pair = pairs[i].split("\\.");
          if (pair.length != 2)
          {
            commandOptions.printUsage(true);
            throw new
              IllegalArgumentException(
                  "Incorrectly-specified segment start and end labels: " +
                  pairs[i]);
          }
          startTags[i] = pair[0];
          continueTags[i] = pair[1];
        }
        eval = new MultiSegmentationEvaluator(new InstanceList[] {trainingData, testData}, new String[] {"Training", "Testing"}, 
        		startTags, continueTags);
      }
      else
      {
        commandOptions.printUsage(true);
        throw new IllegalArgumentException("Invalid test option: " +
            testOption.value);
      }
    }
    
    
    
    if (p.isTargetProcessing())
    {
      Alphabet targets = p.getTargetAlphabet();
      StringBuffer buf = new StringBuffer("Labels:");
      for (int i = 0; i < targets.size(); i++)
        buf.append(" ").append(targets.lookupObject(i).toString());
      logger.info(buf.toString());
    }
    if (trainOption.value)
    {
      crf = train(trainingData, testData, eval,
          ordersOption.value, defaultOption.value,
          forbiddenOption.value, allowedOption.value,
          connectedOption.value, iterationsOption.value,
          gaussianVarianceOption.value, crf);
      if (modelOption.value != null)
      {
        ObjectOutputStream s =
          new ObjectOutputStream(new FileOutputStream(modelOption.value));
        s.writeObject(crf);
        s.close();
      }
    }
    else
    {
      if (crf == null)
      {
        if (modelOption.value == null)
        {
          commandOptions.printUsage(true);
          throw new IllegalArgumentException("Missing model file option");
        }
        ObjectInputStream s =
          new ObjectInputStream(new FileInputStream(modelOption.value));
        crf = (CRF) s.readObject();
        s.close();
      }
      if (eval != null)
        test(new NoopTransducerTrainer(crf), eval, testData);
      else
      {
        boolean includeInput = includeInputOption.value();
        for (int i = 0; i < testData.size(); i++)
        {
          Sequence input = (Sequence)testData.get(i).getData();
          Sequence[] outputs = apply(crf, input, nBestOption.value);
          int k = outputs.length;
          boolean error = false;
          for (int a = 0; a < k; a++) {
            if (outputs[a].size() != input.size()) {
              logger.info("Failed to decode input sequence " + i + ", answer " + a);
              error = true;
            }
          }
          if (!error) {
        	

        	
            for (int j = 0; j < input.size(); j++)
            {
               StringBuffer buf = new StringBuffer();
              for (int a = 0; a < k; a++)
                 buf.append(outputs[a].get(j).toString()).append(" ");
              if (includeInput) {
                FeatureVector fv = (FeatureVector)input.get(j);
                buf.append(fv.toString(true));                
              }
              
              //	Count how many times slacker/aggressive/panicky language is used
              
              String output=buf.toString();
        	  
              if(output.charAt(0)=='S'){
            	Sla++;
              }
              
              if(output.charAt(0)=='A'){
              	Agg++;  
                }
              
              if(output.charAt(0)=='P'){
                	Pan++;  
                  }
            }
            //System.out.println();
            
          }
          
        }
      }
    }
    
    //	Print counts of language types to console
    
    System.out.println("Slang count "+Sla+", Aggression count "+Agg+ ", Panic count "+Pan);
    
    //	Add to Arraylists for use in plots
    
    sla.add(Sla);
    agg.add(Agg);
    pan.add(Pan);
    
    //***********	2. TOPIC MODEL TRAINING SECTION
    
    test tm = new test();
	tm.buildTopicModel("ti"+f);
	
	//	Write tagging results to file  
	  
	String filename = "to"+f;
	BufferedWriter output = new BufferedWriter(new FileWriter(filename));
	
	Iterator<Integer> keySetIterator = topicwords.keySet().iterator();

	while(keySetIterator.hasNext()){
	  Integer key = keySetIterator.next();
	  output.write(key+" "+topicwords.get(key)+"\n");
	}
	
	//	Close output file
	
	output.close();
    
  }
	  	// Create comparative line chart  
	  
	    Chart2D chart = new Chart2D();
	    
	    // Create a ITraces for slacker language; aggressive language; panicky language; stock price

	    ITrace2D trace1 = new Trace2DSimple();
	    trace1.setColor(Color.RED);
	    trace1.setName("Slacker language");
	    
	    ITrace2D trace2 = new Trace2DSimple();
	    trace2.setColor(Color.green);
	    trace2.setName("Aggressive language");
	    
	    ITrace2D trace3 = new Trace2DSimple();
	    trace3.setColor(Color.magenta);
	    trace3.setName("Panicky language");
	    
	    ITrace2D trace4 = new Trace2DSimple();
	    trace4.setColor(Color.BLUE);
	    trace4.setName("Enron stock price");
	    
	    // Add traces to the chart.
	    
	    chart.addTrace(trace1);
	    chart.addTrace(trace2);
	    chart.addTrace(trace3);
	    chart.addTrace(trace4);
	    
	    for(int i=0;i<sla.size();i++){
	      trace1.addPoint(i,sla.get(i));
	      trace2.addPoint(i,agg.get(i));
	      trace3.addPoint(i,pan.get(i)); 
	    }
	    
	    //	Stock price trace is different length to others
	    //	so needs another loops
	    
	    for(int i=0;i<stock.size();i++){
	    trace4.addPoint(i,stock.get(i));
	    }
	    
	    // Make it visible:
	    // Create a frame.
	    JFrame frame = new JFrame("Comparative plot; slacker language vs aggressive language vs panicky language vs stock prices");
	    // add the chart to the frame: 
	    frame.getContentPane().add(chart);
	    frame.setSize(600,600);
	    
	    //**************	3.	Calculate correlations
	    //	Need means, std's, sum of squares, standardised values 
	    
	    Double SLAStd = 0.0;
	    Double SLAMean = 0.0;
	    Double SLASum = 0.0;
	    Double AGGStd = 0.0;
	    Double AGGMean = 0.0;
	    Double AGGSum = 0.0;
	    Double PANStd = 0.0;
	    Double PANMean = 0.0;
	    Double PANSum = 0.0;
	    Double StockStd = 0.0;
	    Double StockSum = 0.0;
	    Double StockMean = 0.0;
	    
	    //	Means
	    //	Slacker phrases
	    
	    for(int i=0;i<sla.size();i++){
	    	SLASum=SLASum+pan.get(i);
	    }
	    
	    SLAMean=SLASum/sla.size();

	    //	Aggressive phrases
	    
	    for(int i=0;i<agg.size();i++){
	    	AGGSum=AGGSum+pan.get(i);
	    }
	    
	    AGGMean=AGGSum/pan.size();
	    
	    //	Panicky phrases
	    
	    for(int i=0;i<pan.size();i++){
	    	PANSum=PANSum+pan.get(i);
	    }
	    
	    PANMean=PANSum/pan.size();
	    
	    //	Stock prices
	    
	    for(int i=0;i<stock.size();i++){
	    	StockSum=StockSum+stock.get(i);
	    }
	    
	    StockMean=StockSum/stock.size();
	    
	    //	Standard deviation calculations
	    //	Instantiate ArrayLists to store sum-of-squares calculations
	    //	Slacker phrase SOS's
	    
	    ArrayList<Double> SLA_SOS = new ArrayList<Double>();
	    
	    Double SLASumSTD=0.0;
	    
	    for(int i=0;i<sla.size();i++){
	    	SLA_SOS.add((sla.get(i)-SLAMean)*(sla.get(i)-SLAMean));
	    }
	    
	    for(int i=0;i<SLA_SOS.size();i++){
	    	SLASumSTD=SLASumSTD+SLA_SOS.get(i);
	    }
	    
	    //	Slacker STD
	    
	    SLAStd = Math.sqrt((1.0/(sla.size()-1))*SLASumSTD);
	    
	    //	Aggressive phrase SOS's
	    
	    ArrayList<Double> AGG_SOS = new ArrayList<Double>();
	    
	    Double AGGSumSTD=0.0;
	    
	    for(int i=0;i<agg.size();i++){
	    	AGG_SOS.add((agg.get(i)-AGGMean)*(agg.get(i)-AGGMean));
	    }
	    
	    for(int i=0;i<AGG_SOS.size();i++){
	    	AGGSumSTD=AGGSumSTD+AGG_SOS.get(i);
	    }
	    
	    //	Aggressive STD
	    
	    AGGStd = Math.sqrt((1.0/(agg.size()-1))*AGGSumSTD);
	    
	    //	Panicky phrase SOS's
	    
	    ArrayList<Double> PAN_SOS = new ArrayList<Double>();
	    
	    Double PANSumSTD=0.0;
	    
	    for(int i=0;i<pan.size();i++){
	    	PAN_SOS.add((pan.get(i)-PANMean)*(pan.get(i)-PANMean));
	    }
	    
	    for(int i=0;i<PAN_SOS.size();i++){
	    	PANSumSTD=PANSumSTD+PAN_SOS.get(i);
	    }
	    
	    //	Panicky STD
	    
	    PANStd = Math.sqrt((1.0/(pan.size()-1))*PANSumSTD);
	    
	    //	Stock price SOS's
	    
	    ArrayList<Double> stock_SOS = new ArrayList<Double>();
	    
	    Double StockSumSTD=0.0;
	    
	    for(int i=0;i<stock.size();i++){
	    	stock_SOS.add((stock.get(i)-StockMean)*(stock.get(i)-StockMean));
	    }
	    
	    for(int i=0;i<stock_SOS.size();i++){
	    	StockSumSTD=StockSumSTD+stock_SOS.get(i);
	    }
	    
	    //	Stock price STD
	    
	    StockStd = Math.sqrt((1.0/(stock.size()-1))*StockSumSTD);
	    
	    System.out.println("Stock mean ="+StockMean+" Stock STD ="+StockStd);
	    System.out.println("Slacker mean ="+SLAMean+" Slacker STD ="+SLAStd);
	    System.out.println("Aggression mean ="+AGGMean+" Aggression STD ="+AGGStd);
	    System.out.println("Panic mean ="+PANMean+" Panic STD ="+PANStd);
	    
	    //	Calculate correlations - instantiate arrays to hold standardised values (by mean)
	    
	    ArrayList<Double> SLA_StdV = new ArrayList<Double>();
	    ArrayList<Double> AGG_StdV = new ArrayList<Double>();
	    ArrayList<Double> PAN_StdV = new ArrayList<Double>();
	    ArrayList<Double> Stock_StdV = new ArrayList<Double>();

	    //	Standardised values
	    
	    for(int i=0;i<pan.size();i++){
	    	SLA_StdV.add((sla.get(i)-SLAMean)/SLAStd);
	    	AGG_StdV.add((agg.get(i)-AGGMean)/AGGStd);
	    	PAN_StdV.add((pan.get(i)-PANMean)/PANStd);
	    }
	    
	    for(int i=0;i<stock.size();i++){
	    	Stock_StdV.add((stock.get(i)-StockMean)/StockStd);
	    }
	    
	    //	Clunky as hell - use ArrayList to store standardised values of which
	    //	variable is being correlated with stock price and clear after
	    //	each run
	    
	    ArrayList<Double> standardisedvalues = new ArrayList<Double>();
	    
	    for(int i=0;i<Stock_StdV.size();i++){
	    	standardisedvalues.add(Stock_StdV.get(i)*SLA_StdV.get(i));
	    }
	    
	    Double sumstdV=0.0;
	    
	    for(int i=0;i<standardisedvalues.size();i++){
	    	sumstdV=sumstdV+standardisedvalues.get(i);
	    }
	    
	    double corrSLA=sumstdV/(24-1);
	    
	    System.out.println("Slacker correlation with stock: "+corrSLA);
	    
	    //	Clear ArrayList
	    standardisedvalues.clear();
	    
	    for(int i=0;i<Stock_StdV.size();i++){
	    	standardisedvalues.add(Stock_StdV.get(i)*AGG_StdV.get(i));
	    }
	    
	    sumstdV=0.0;
	    
	    for(int i=0;i<standardisedvalues.size();i++){
	    	sumstdV=sumstdV+standardisedvalues.get(i);
	    }
	    
	    double corrAGG=sumstdV/(24-1);
	    System.out.println("Aggression correlation with stock: "+corrAGG);
	    
	    //	Clear ArrayList
	    standardisedvalues.clear();
	    
	    for(int i=0;i<Stock_StdV.size();i++){
	    	standardisedvalues.add(Stock_StdV.get(i)*PAN_StdV.get(i));
	    }
	    
	    sumstdV=0.0;
	    
	    for(int i=0;i<standardisedvalues.size();i++){
	    	sumstdV=sumstdV+standardisedvalues.get(i);
	    }
	    
	    double corrPAN=sumstdV/(24-1);
	    System.out.println("Panic correlation with stock: "+corrPAN);
	    
	    //	Clear ArrayList
	    standardisedvalues.clear();
	    
	    
	    
	    // Enable the termination button [cross on the upper right edge]: 
	    frame.addWindowListener(
	        new WindowAdapter(){
	          public void windowClosing(WindowEvent e){
	              System.exit(0);
	          }
	        }
	      );
	    frame.setVisible(true);
	    
	    //	Display correlations as dialog box.
	    
	    JOptionPane.showMessageDialog(frame, "Slacker phrase correlation with stock: "+corrSLA+"\nAggression phrase correlation with stock: "+corrAGG+"\nPanic phrase correlation with stock: "+corrPAN);
	    
	    //************	4.	File viewer to view text files outputted by topic model
	    
	    Frame f = new FileViewer((args.length == 1) ? args[0] : null);
	    
	    f.addWindowListener(new WindowAdapter() {
	      public void windowClosed(WindowEvent e) {
	        System.exit(0);
	      }
	    });
	    
	    f.show();
  }
  
  //	TRAINING OF TOPIC MODEL
  
  public void buildTopicModel(String fileName) throws Exception {
		// Begin by importing documents from text to feature sequences
				ArrayList<Pipe> pipeList = new ArrayList<Pipe>();

				// Pipes: lowercase, tokenize, remove stopwords, map to features
				pipeList.add( new CharSequenceLowercase() );
				pipeList.add( new CharSequence2TokenSequence(Pattern.compile("\\p{L}[\\p{L}\\p{P}]+\\p{L}")) );
				pipeList.add( new TokenSequenceRemoveStopwords(new File("en.txt"), "UTF-8", false, false, false) );
				pipeList.add( new TokenSequence2FeatureSequence() );

				instances = new InstanceList (new SerialPipes(pipeList));

				Reader fileReader = new InputStreamReader(new FileInputStream(new File(fileName)), "UTF-8");
				instances.addThruPipe(new CsvIterator (fileReader, Pattern.compile("^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$"),
													   3, 2, 1)); // data, label, name fields

				// Create a model with 100 topics, alpha_t = 0.01, beta_w = 0.01
				//  Note that the first parameter is passed as the sum over topics, while
				//  the second is 
				int numTopics = 10;
				model = new ParallelTopicModel(numTopics, 1.0, 0.01);

				model.addInstances(instances);

				// Use two parallel samplers, which each look at one half the corpus and combine
				//  statistics after every iteration.
				model.setNumThreads(2);

				// Run the model for 50 iterations and stop (this is for testing only, 
				//  for real applications, use 1000 to 2000 iterations)
				model.setNumIterations(50);
				model.estimate();

				// Show the words and topics in the first instance

				// The data alphabet maps word IDs to strings
				Alphabet dataAlphabet = instances.getDataAlphabet();
				
				FeatureSequence tokens = (FeatureSequence) model.getData().get(0).instance.getData();
				LabelSequence topics = model.getData().get(0).topicSequence;
				
				Formatter out = new Formatter(new StringBuilder(), Locale.US);
				for (int position = 0; position < tokens.getLength(); position++) {
					out.format("%s-%d ", dataAlphabet.lookupObject(tokens.getIndexAtPosition(position)), topics.getIndexAtPosition(position));
				}
				
				// Estimate the topic distribution of the first instance, 
				//  given the current Gibbs state.
				double[] topicDistribution = model.getTopicProbabilities(topics);
				
				// Get an array of sorted sets of word ID/count pairs
		        ArrayList<TreeSet<IDSorter>> topicSortedWords = model.getSortedWords();
		        
				  // Show top 5 words in topics with proportions for the first document
		        
		        for (int topic = 0; topic < numTopics; topic++) {
		            Iterator<IDSorter> iterator = topicSortedWords.get(topic).iterator();
		            
		            out = new Formatter(new StringBuilder(), Locale.US);
		            out.format("%d\t%.3f\t", topic, topicDistribution[topic]);
		            int rank = 0;
		            while (iterator.hasNext() && rank < 10) {
		                IDSorter idCountPair = iterator.next();
		                out.format("%s %.0f ", dataAlphabet.lookupObject(idCountPair.getID()), idCountPair.getWeight());
		                rank++;
		            }
		            
		            //	Put all topics and words into a container for querying later
		            
		            String[] words = out.toString().split("\t");
		            topicwords.put(Integer.valueOf(words[0]), words[2]);
		        }
	}
}
